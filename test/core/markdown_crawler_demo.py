#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MarkdownCrawler æ¼”ç¤ºå’Œå¿«é€Ÿæµ‹è¯•è„šæœ¬
ç”¨äºå±•ç¤ºMarkdownCrawlerçš„ä¸»è¦åŠŸèƒ½å’Œä½¿ç”¨æ–¹æ³•
"""

import asyncio
import sys
from pathlib import Path
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from fnewscrawler.spiders.core.markdown_crawler import MarkdownCrawler


async def demo_basic_usage():
    """æ¼”ç¤ºåŸºæœ¬ä½¿ç”¨æ–¹æ³•"""
    print("\nğŸ”¥ æ¼”ç¤º1: åŸºæœ¬ä½¿ç”¨æ–¹æ³•")
    print("=" * 50)
    
    # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼ˆæ¨èæ–¹å¼ï¼‰
    async with MarkdownCrawler() as crawler:
        result = await crawler.get_markdown_content(
            "https://finance.eastmoney.com/a/202507233464766684.html",
            content_threshold=0.4,
            use_cache=True
        )
        
        print(f"\nğŸ“Š çˆ¬å–ç»“æœ:")
        print(f"   æˆåŠŸ: {result['success']}")
        print(f"   URL: {result['url']}")
        print(f"   çŠ¶æ€ç : {result['status_code']}")
        print(f"   æ¸…ç†åMarkdowné•¿åº¦: {len(result['markdown'])}")
        print(f"   å­—æ•°ç»Ÿè®¡: {result['word_count']}")
        
        print(result['markdown'])
        # if result['success'] and result['markdown']:
        #     preview = result['markdown'][:300].replace('\n', ' ')
        #     print(f"\nğŸ“ å†…å®¹é¢„è§ˆ:\n{preview}...")
        
        if result['error']:
            print(f"é”™è¯¯ä¿¡æ¯: {result['error']}")


async def demo_quick_crawl():
    """æ¼”ç¤ºå¿«é€Ÿçˆ¬å–æ–¹æ³•"""
    print("\nâš¡ æ¼”ç¤º2: å¿«é€Ÿçˆ¬å–æ–¹æ³•")
    print("=" * 50)
    
    # å¿«é€Ÿçˆ¬å–å•ä¸ªURL
    result = await MarkdownCrawler.quick_crawl(
        "https://httpbin.org/json",
        content_threshold=0.3
    )
    
    print(f"\nğŸ“Š å¿«é€Ÿçˆ¬å–ç»“æœ:")
    print(f"   æˆåŠŸ: {result['success']}")
    print(f"   å­—æ•°: {result['word_count']}")
    
    if result['success'] and result['markdown']:
        preview = result['markdown'][:200].replace('\n', ' ')
        print(f"   å†…å®¹é¢„è§ˆ: {preview}...")


async def demo_batch_crawl():
    """æ¼”ç¤ºæ‰¹é‡çˆ¬å–"""
    print("\nğŸ“¦ æ¼”ç¤º3: æ‰¹é‡çˆ¬å–")
    print("=" * 50)
    
    urls = [
        "https://httpbin.org/html",
        "https://httpbin.org/json",
        "https://httpbin.org/xml",
        "https://httpbin.org/robots.txt"
    ]
    
    start_time = time.time()
    
    async with MarkdownCrawler() as crawler:
        results = await crawler.batch_get_markdown(
            urls,
            max_concurrent=2,
            content_threshold=0.3
        )
    
    end_time = time.time()
    elapsed = end_time - start_time
    
    print(f"æ‰¹é‡çˆ¬å–å®Œæˆï¼Œè€—æ—¶: {elapsed:.2f}ç§’")
    print(f"å¤„ç†URLæ•°: {len(urls)}")
    
    successful_count = 0
    total_words = 0
    
    for i, result in enumerate(results):
        status = "âœ…" if result['success'] else "âŒ"
        print(f"   {status} URL {i+1}: {result['word_count']} å­—")
        
        if result['success'] and result['markdown']:
            preview = result['markdown'][:100].replace('\n', ' ')
            print(f"      é¢„è§ˆ: {preview}...")
        
        if result['success']:
            successful_count += 1
            total_words += result['word_count']
        
        if result['error']:
            print(f"    é”™è¯¯: {result['error']}")
    
    print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  æˆåŠŸç‡: {successful_count}/{len(urls)} ({successful_count/len(urls)*100:.1f}%)")
    print(f"  æ€»å­—æ•°: {total_words}")
    print(f"  å¹³å‡é€Ÿåº¦: {total_words/elapsed:.0f} å­—/ç§’")


async def demo_singleton_pattern():
    """æ¼”ç¤ºå•ä¾‹æ¨¡å¼"""
    print("\nğŸ”„ æ¼”ç¤º4: å•ä¾‹æ¨¡å¼éªŒè¯")
    print("=" * 50)
    
    # åˆ›å»ºå¤šä¸ªå®ä¾‹
    crawler1 = MarkdownCrawler()
    crawler2 = MarkdownCrawler()
    crawler3 = MarkdownCrawler()
    
    print(f"crawler1 ID: {id(crawler1)}")
    print(f"crawler2 ID: {id(crawler2)}")
    print(f"crawler3 ID: {id(crawler3)}")
    
    print(f"æ˜¯å¦ä¸ºåŒä¸€å®ä¾‹: {crawler1 is crawler2 is crawler3}")
    
    if crawler1 is crawler2 is crawler3:
        print("âœ… å•ä¾‹æ¨¡å¼å·¥ä½œæ­£å¸¸")
    else:
        print("âŒ å•ä¾‹æ¨¡å¼å¼‚å¸¸")


async def demo_error_handling():
    """æ¼”ç¤ºé”™è¯¯å¤„ç†"""
    print("\nğŸš¨ æ¼”ç¤º5: é”™è¯¯å¤„ç†")
    print("=" * 50)
    
    # æµ‹è¯•æ— æ•ˆURL
    invalid_urls = [
        "https://this-domain-does-not-exist-12345.com",
        "http://invalid-url",
        "not-a-url"
    ]
    
    async with MarkdownCrawler() as crawler:
        for url in invalid_urls:
            print(f"\næµ‹è¯•æ— æ•ˆURL: {url}")
            
            result = await crawler.get_markdown_content(url)
            
            print(f"\nğŸ“Š é”™è¯¯å¤„ç†ç»“æœ:")
            print(f"   æˆåŠŸ: {result['success']}")
            print(f"   é”™è¯¯ä¿¡æ¯: {result['error']}")
            print(f"   Markdownå†…å®¹: '{result['markdown']}'")
            print(f"   å­—æ•°: {result['word_count']}")


async def demo_custom_config():
    """æ¼”ç¤ºè‡ªå®šä¹‰é…ç½®"""
    print("\nâš™ï¸ æ¼”ç¤º6: è‡ªå®šä¹‰é…ç½®")
    print("=" * 50)
    
    test_url = "https://httpbin.org/html"
    
    # ä¸åŒçš„é…ç½®å‚æ•°
    configs = [
        {
            "name": "é«˜è´¨é‡è¿‡æ»¤",
            "params": {
                "content_threshold": 0.7,
                "min_word_threshold": 20,
                "exclude_external_links": True
            }
        },
        {
            "name": "å®½æ¾è¿‡æ»¤",
            "params": {
                "content_threshold": 0.2,
                "min_word_threshold": 5,
                "exclude_external_links": False
            }
        },
        {
            "name": "æ— ç¼“å­˜æ¨¡å¼",
            "params": {
                "use_cache": False,
                "content_threshold": 0.4
            }
        }
    ]
    
    async with MarkdownCrawler() as crawler:
        for config in configs:
            print(f"\né…ç½®: {config['name']}")
            
            result = await crawler.get_markdown_content(
                test_url,
                **config['params']
            )
            
            print(f"\nğŸ“Š è‡ªå®šä¹‰é…ç½®ç»“æœ:")
            print(f"   æˆåŠŸ: {result['success']}")
            print(f"   å­—æ•°: {result['word_count']}")
            
            if result['success'] and result['markdown']:
                preview = result['markdown'][:150].replace('\n', ' ')
                print(f"   å†…å®¹é¢„è§ˆ: {preview}...")


async def demo_performance_comparison():
    """æ¼”ç¤ºæ€§èƒ½å¯¹æ¯”"""
    print("\nğŸƒ æ¼”ç¤º7: æ€§èƒ½å¯¹æ¯”")
    print("=" * 50)
    
    test_urls = [
        "https://httpbin.org/html",
        "https://httpbin.org/json",
        "https://httpbin.org/xml"
    ]
    
    # æµ‹è¯•1: é¡ºåºçˆ¬å–
    print("\né¡ºåºçˆ¬å–æµ‹è¯•:")
    start_time = time.time()
    
    async with MarkdownCrawler() as crawler:
        sequential_results = []
        for url in test_urls:
            result = await crawler.get_markdown_content(url)
            sequential_results.append(result)
    
    sequential_time = time.time() - start_time
    sequential_words = sum(r['word_count'] for r in sequential_results if r['success'])
    
    # éªŒè¯è´¢ç»æ–°é—»å†…å®¹æ¸…ç†
    for result in sequential_results:
        if result['success']:
            # éªŒè¯é“¾æ¥å·²è¢«ç§»é™¤
            assert '[' not in result['markdown'] or '](' not in result['markdown']
    
    # æµ‹è¯•2: å¹¶å‘çˆ¬å–
    print("\nå¹¶å‘çˆ¬å–æµ‹è¯•:")
    start_time = time.time()
    
    async with MarkdownCrawler() as crawler:
        concurrent_results = await crawler.batch_get_markdown(
            test_urls,
            max_concurrent=3
        )
    
    concurrent_time = time.time() - start_time
    concurrent_words = sum(r['word_count'] for r in concurrent_results if r['success'])
    
    # éªŒè¯å¹¶å‘ç»“æœçš„è´¢ç»æ–°é—»æ¸…ç†
    for result in concurrent_results:
        if result['success']:
            assert '[' not in result['markdown'] or '](' not in result['markdown']
    
    print(f"\nğŸ“Š æ€§èƒ½å¯¹æ¯”ç»“æœ:")
    print(f"  é¡ºåºçˆ¬å–: {sequential_time:.2f}ç§’, {sequential_words}å­—")
    print(f"  å¹¶å‘çˆ¬å–: {concurrent_time:.2f}ç§’, {concurrent_words}å­—")
    if concurrent_time > 0:
        print(f"  æ€§èƒ½æå‡: {sequential_time/concurrent_time:.1f}å€")
    print(f"  âœ… è´¢ç»æ–°é—»å†…å®¹æ¸…ç†éªŒè¯é€šè¿‡ï¼ˆå·²ç§»é™¤æ‰€æœ‰é“¾æ¥ï¼‰")
    
    # éªŒè¯ç»“æœä¸€è‡´æ€§
    sequential_success = sum(1 for r in sequential_results if r['success'])
    concurrent_success = sum(1 for r in concurrent_results if r['success'])
    
    print(f"  é¡ºåºçˆ¬å–æˆåŠŸæ•°: {sequential_success}/{len(test_urls)}")
    print(f"  å¹¶å‘çˆ¬å–æˆåŠŸæ•°: {concurrent_success}/{len(test_urls)}")
    
    # éªŒè¯ç»“æœä¸€è‡´æ€§
    print("\nğŸ“Š ç»“æœä¸€è‡´æ€§éªŒè¯:")
    for i, (seq_result, conc_result) in enumerate(zip(sequential_results, concurrent_results)):
        seq_success = seq_result['success']
        conc_success = conc_result['success']
        print(f"   URL {i+1}: é¡ºåº={seq_success}, å¹¶å‘={conc_success}")
        
        if seq_success and conc_success:
            # æ¯”è¾ƒå†…å®¹é•¿åº¦å·®å¼‚
            seq_len = len(seq_result.get('markdown', ''))
            conc_len = len(conc_result.get('markdown', ''))
            diff_percent = abs(seq_len - conc_len) / max(seq_len, conc_len, 1) * 100
            print(f"      å†…å®¹é•¿åº¦å·®å¼‚: {diff_percent:.1f}%")


async def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("ğŸ¯ MarkdownCrawler åŠŸèƒ½æ¼”ç¤º")
    print("=" * 60)
    print("è¿™ä¸ªæ¼”ç¤ºå°†å±•ç¤ºMarkdownCrawlerçš„å„ç§åŠŸèƒ½å’Œä½¿ç”¨æ–¹æ³•")
    
    try:
        # è¿è¡Œæ‰€æœ‰æ¼”ç¤º
        await demo_basic_usage()
        await demo_quick_crawl()
        await demo_batch_crawl()
        await demo_singleton_pattern()
        await demo_error_handling()
        await demo_custom_config()
        await demo_performance_comparison()
        
        print("\nğŸ‰ æ‰€æœ‰æ¼”ç¤ºå®Œæˆï¼")
        print("\nğŸ’¡ ä½¿ç”¨æç¤º:")
        print("  1. æ¨èä½¿ç”¨ 'async with MarkdownCrawler() as crawler' è¯­æ³•")
        print("  2. æ‰¹é‡çˆ¬å–æ—¶æ³¨æ„æ§åˆ¶å¹¶å‘æ•°ï¼Œé¿å…å¯¹ç›®æ ‡ç½‘ç«™é€ æˆå‹åŠ›")
        print("  3. æ ¹æ®éœ€è¦è°ƒæ•´ content_threshold å‚æ•°æ¥æ§åˆ¶å†…å®¹è´¨é‡")
        print("  4. å¯ç”¨ç¼“å­˜å¯ä»¥æ˜¾è‘—æå‡é‡å¤è¯·æ±‚çš„æ€§èƒ½")
        print("  5. å¤„ç†è´¢ç»æ–°é—»æ—¶ï¼Œå»ºè®®è®¾ç½®åˆé€‚çš„ excluded_tags")
        
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        print("è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œä¾èµ–åº“æ˜¯å¦æ­£ç¡®å®‰è£…")


if __name__ == "__main__":
    print("å¯åŠ¨MarkdownCrawleræ¼”ç¤º...")
    asyncio.run(main())