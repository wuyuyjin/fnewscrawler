from io import StringIO
from typing import Dict, Any, List, Optional, Union

import pandas as pd

from fnewscrawler.core.context import context_manager
from fnewscrawler.utils.logger import LOGGER


async def extract_structured_data(
    url: str,
    css_selector: str,
    context_name: str = "default",
    extract_type: str = "text",
    wait_for_selector: Optional[str] = None,
    wait_timeout: int = 10000,
    multiple: bool = True,
    attributes: Optional[List[str]] = None,
    format_for_llm: bool = True
) -> Dict[str, Any]:
    """
    ä»æŒ‡å®šURLå’ŒCSSé€‰æ‹©å™¨ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯ï¼Œä¸“ä¸ºAIå¤§è¯­è¨€æ¨¡å‹ä¼˜åŒ–

    Args:
        url (str): è¦çˆ¬å–çš„ç½‘é¡µURL
        css_selector (str): CSSé€‰æ‹©å™¨ï¼Œç”¨äºå®šä½è¦æå–çš„å…ƒç´ 
        context_name (str): æµè§ˆå™¨contextåç§°ï¼Œç”¨äºä¼šè¯ç®¡ç†ï¼Œé»˜è®¤ä¸º"default"
        extract_type (str): æå–ç±»å‹ï¼Œå¯é€‰ "text", "html", "attribute", "mixed"ï¼Œé»˜è®¤ä¸º"text"
        wait_for_selector (str, optional): ç­‰å¾…å‡ºç°çš„ç‰¹å®šé€‰æ‹©å™¨ï¼Œç¡®ä¿åŠ¨æ€å†…å®¹åŠ è½½å®Œæˆ
        wait_timeout (int): ç­‰å¾…è¶…æ—¶æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ï¼Œé»˜è®¤10000
        multiple (bool): æ˜¯å¦æå–å¤šä¸ªå…ƒç´ ï¼ŒTrueä¸ºæå–æ‰€æœ‰åŒ¹é…çš„å…ƒç´ ï¼ŒFalseä¸ºåªæå–ç¬¬ä¸€ä¸ª
        attributes (List[str], optional): å½“extract_typeä¸º"attribute"æ—¶ï¼ŒæŒ‡å®šè¦æå–çš„å±æ€§åˆ—è¡¨
        format_for_llm (bool): æ˜¯å¦å°†æ•°æ®æ ¼å¼åŒ–ä¸ºä¾¿äºå¤§è¯­è¨€æ¨¡å‹ç†è§£çš„æ–‡æœ¬æ ¼å¼ï¼Œé»˜è®¤ä¸ºTrue

    Returns:
        Dict[str, Any]: åŒ…å«æå–ç»“æœçš„å­—å…¸
            - success (bool): æ˜¯å¦æˆåŠŸæå–æ•°æ®
            - data (Union[str, List[Dict]]): æå–çš„æ•°æ®ï¼Œæ ¹æ®format_for_llmå‚æ•°è¿”å›ä¸åŒæ ¼å¼
            - raw_data (List[Dict]): åŸå§‹ç»“æ„åŒ–æ•°æ®ï¼ˆå½“format_for_llm=Trueæ—¶æä¾›ï¼‰
            - count (int): æå–çš„å…ƒç´ æ•°é‡
            - url (str): æºURL
            - selector (str): ä½¿ç”¨çš„CSSé€‰æ‹©å™¨
            - extract_type (str): ä½¿ç”¨çš„æå–ç±»å‹
            - message (str): çŠ¶æ€æ¶ˆæ¯
            - formatted_summary (str): ä¸ºAIæ¨¡å‹å‡†å¤‡çš„æ ¼å¼åŒ–æ‘˜è¦ï¼ˆå½“format_for_llm=Trueæ—¶æä¾›ï¼‰

    Raises:
        Exception: å½“ç½‘é¡µè®¿é—®å¤±è´¥æˆ–æ•°æ®æå–å‡ºé”™æ—¶æŠ›å‡º
    """
    try:
        LOGGER.info(f"å¼€å§‹ä» {url} æå–æ•°æ®ï¼Œé€‰æ‹©å™¨: {css_selector}, context: {context_name}")

        # è·å–æˆ–åˆ›å»ºæµè§ˆå™¨ä¸Šä¸‹æ–‡
        context = await context_manager.get_context(context_name)
        page = await context.new_page()

        # è®¾ç½®ç”¨æˆ·ä»£ç†
        await page.set_extra_http_headers({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        })

        result_data = {
            "success": False,
            "data": [] if multiple else "",
            "raw_data": [],
            "count": 0,
            "url": url,
            "selector": css_selector,
            "extract_type": extract_type,
            "message": "",
            "formatted_summary": ""
        }

        try:
            # è®¿é—®ç›®æ ‡ç½‘é¡µ
            LOGGER.info(f"æ­£åœ¨è®¿é—®ç½‘é¡µ: {url}")
            await page.goto(url, wait_until="domcontentloaded", timeout=30000)

            # å¦‚æœæŒ‡å®šäº†ç­‰å¾…çš„é€‰æ‹©å™¨ï¼Œç­‰å¾…å…¶å‡ºç°
            if wait_for_selector:
                LOGGER.info(f"ç­‰å¾…é€‰æ‹©å™¨å‡ºç°: {wait_for_selector}")
                try:
                    await page.wait_for_selector(wait_for_selector, timeout=wait_timeout)
                except Exception as e:
                    LOGGER.warning(f"ç­‰å¾…é€‰æ‹©å™¨è¶…æ—¶: {wait_for_selector}, é”™è¯¯: {e}")
                    # ç»§ç»­æ‰§è¡Œï¼Œä¸æŠ›å‡ºå¼‚å¸¸

            # å®šä½ç›®æ ‡å…ƒç´ 
            elements = page.locator(css_selector)
            element_count = await elements.count()

            if element_count == 0:
                result_data["message"] = f"æœªæ‰¾åˆ°åŒ¹é…é€‰æ‹©å™¨ '{css_selector}' çš„å…ƒç´ "
                LOGGER.warning(result_data["message"])
                return result_data

            LOGGER.info(f"æ‰¾åˆ° {element_count} ä¸ªåŒ¹é…çš„å…ƒç´ ")

            # æå–æ•°æ®
            extracted_data = []

            if multiple:
                # æå–æ‰€æœ‰åŒ¹é…çš„å…ƒç´ 
                for i in range(element_count):
                    element = elements.nth(i)
                    element_data = await _extract_element_data(
                        element, extract_type, attributes, i
                    )
                    if element_data:
                        extracted_data.append(element_data)
            else:
                # åªæå–ç¬¬ä¸€ä¸ªå…ƒç´ 
                element = elements.first
                element_data = await _extract_element_data(
                    element, extract_type, attributes, 0
                )
                extracted_data = element_data if isinstance(element_data, dict) else [element_data]

            # è®¾ç½®è¿”å›ç»“æœ
            result_data["success"] = True
            result_data["raw_data"] = extracted_data if isinstance(extracted_data, list) else [extracted_data]
            result_data["count"] = element_count
            result_data["message"] = f"æˆåŠŸæå– {element_count} ä¸ªå…ƒç´ çš„æ•°æ®"

            # æ ¹æ®format_for_llmå‚æ•°æ ¼å¼åŒ–æ•°æ®
            if format_for_llm:
                result_data["formatted_summary"] = _format_data_for_llm(extracted_data, extract_type, url, css_selector)
                result_data["data"] = result_data["formatted_summary"] if multiple else (result_data["formatted_summary"] if extracted_data else "")
            else:
                result_data["data"] = extracted_data if multiple else (extracted_data[0] if extracted_data else "")

            LOGGER.info(result_data["message"])

        finally:
            # å…³é—­é¡µé¢
            await page.close()

        return result_data

    except Exception as e:
        error_msg = f"æå–æ•°æ®å¤±è´¥: {str(e)}"
        LOGGER.error(error_msg)

        return {
            "success": False,
            "data": "" if format_for_llm else ([] if multiple else ""),
            "raw_data": [],
            "count": 0,
            "url": url,
            "selector": css_selector,
            "extract_type": extract_type,
            "message": error_msg,
            "formatted_summary": ""
        }


async def _extract_element_data(
    element,
    extract_type: str,
    attributes: Optional[List[str]],
    index: int
) -> Union[Dict[str, Any], str, None]:
    """
    æå–å•ä¸ªå…ƒç´ çš„æ•°æ®

    Args:
        element: Playwrightå…ƒç´ å¯¹è±¡
        extract_type (str): æå–ç±»å‹
        attributes (List[str], optional): è¦æå–çš„å±æ€§åˆ—è¡¨
        index (int): å…ƒç´ ç´¢å¼•

    Returns:
        Union[Dict[str, Any], str, None]: æå–çš„æ•°æ®
    """
    try:
        if extract_type == "text":
            return await element.text_content()

        elif extract_type == "html":
            return await element.inner_html()

        elif extract_type == "attribute":
            if not attributes:
                # å¦‚æœæ²¡æœ‰æŒ‡å®šå±æ€§ï¼Œæå–æ‰€æœ‰å±æ€§
                attrs = await element.get_attribute_keys()
                element_data = {"index": index}
                for attr in attrs:
                    value = await element.get_attribute(attr)
                    element_data[attr] = value
                return element_data
            else:
                # æå–æŒ‡å®šå±æ€§
                element_data = {"index": index}
                for attr in attributes:
                    value = await element.get_attribute(attr)
                    element_data[attr] = value
                return element_data

        elif extract_type == "mixed":
            # æ··åˆæå–ï¼šæ–‡æœ¬ã€HTMLå’Œå±æ€§
            element_data = {
                "index": index,
                "text": await element.text_content(),
                "html": await element.inner_html()
            }

            # æå–å¸¸ç”¨å±æ€§
            common_attrs = ["href", "src", "alt", "title", "class", "id"]
            for attr in common_attrs:
                value = await element.get_attribute(attr)
                if value is not None:
                    element_data[attr] = value

            # å¦‚æœæŒ‡å®šäº†é¢å¤–å±æ€§ï¼Œä¹Ÿæå–
            if attributes:
                for attr in attributes:
                    if attr not in common_attrs:
                        value = await element.get_attribute(attr)
                        if value is not None:
                            element_data[attr] = value

            return element_data

        else:
            LOGGER.warning(f"ä¸æ”¯æŒçš„æå–ç±»å‹: {extract_type}")
            return None

    except Exception as e:
        LOGGER.error(f"æå–å…ƒç´ æ•°æ®å¤±è´¥ (ç´¢å¼•: {index}): {str(e)}")
        return None


def _format_data_for_llm(
    data: Union[List[Dict], Dict, str],
    extract_type: str,
    url: str,
    css_selector: str
) -> str:
    """
    å°†æå–çš„æ•°æ®æ ¼å¼åŒ–ä¸ºä¾¿äºå¤§è¯­è¨€æ¨¡å‹ç†è§£çš„æ–‡æœ¬æ ¼å¼

    Args:
        data: æå–çš„åŸå§‹æ•°æ®
        extract_type (str): æå–ç±»å‹
        url (str): æºURL
        css_selector (str): CSSé€‰æ‹©å™¨

    Returns:
        str: æ ¼å¼åŒ–åçš„æ–‡æœ¬
    """
    if not data:
        return "æœªæ‰¾åˆ°åŒ¹é…çš„æ•°æ®"

    # æ„å»ºæ ‡é¢˜ä¿¡æ¯
    summary_parts = [
        f"ç½‘é¡µæ•°æ®æå–ç»“æœ",
        f"æ¥æºURL: {url}",
        f"é€‰æ‹©å™¨: {css_selector}",
        f"æå–ç±»å‹: {extract_type}",
        "-" * 50
    ]

    if isinstance(data, str):
        # ç®€å•æ–‡æœ¬å†…å®¹
        summary_parts.append(f"æå–å†…å®¹:\n{data}")

    elif isinstance(data, list):
        # åˆ—è¡¨æ•°æ®
        if len(data) == 1:
            # å•ä¸ªå…ƒç´ 
            item = data[0]
            if isinstance(item, str):
                summary_parts.append(f"æå–å†…å®¹:\n{item}")
            elif isinstance(item, dict):
                summary_parts.append(f"æå–å†…å®¹:\n{_format_dict_for_llm(item)}")
        else:
            # å¤šä¸ªå…ƒç´ 
            summary_parts.append(f"å…±æ‰¾åˆ° {len(data)} ä¸ªåŒ¹é…å…ƒç´ :")

            for i, item in enumerate(data, 1):
                summary_parts.append(f"\nã€å…ƒç´  {i}ã€‘")
                if isinstance(item, str):
                    summary_parts.append(item)
                elif isinstance(item, dict):
                    summary_parts.append(_format_dict_for_llm(item))

    elif isinstance(data, dict):
        # å­—å…¸æ•°æ®
        summary_parts.append(f"æå–å†…å®¹:\n{_format_dict_for_llm(data)}")

    return "\n".join(summary_parts)


def _format_dict_for_llm(data: Dict[str, Any]) -> str:
    """
    å°†å­—å…¸æ•°æ®æ ¼å¼åŒ–ä¸ºä¾¿äºLLMé˜…è¯»çš„æ–‡æœ¬

    Args:
        data: å­—å…¸æ•°æ®

    Returns:
        str: æ ¼å¼åŒ–åçš„æ–‡æœ¬
    """
    if not data:
        return "ç©ºæ•°æ®"

    formatted_lines = []

    # ç‰¹æ®Šå¤„ç†åŒ…å«textå­—æ®µçš„æƒ…å†µ
    if "text" in data:
        text_content = data["text"]
        if text_content and text_content.strip():
            formatted_lines.append(f"æ–‡æœ¬å†…å®¹: {text_content.strip()}")

        # å¤„ç†å…¶ä»–å­—æ®µ
        other_fields = {k: v for k, v in data.items() if k != "text"}
        if other_fields:
            formatted_lines.append("å…¶ä»–å±æ€§:")
            for key, value in other_fields.items():
                if key != "index":  # è·³è¿‡ç´¢å¼•å­—æ®µ
                    formatted_lines.append(f"  {key}: {value}")
    else:
        # å¸¸è§„å­—å…¸å¤„ç†
        for key, value in data.items():
            if key != "index":  # è·³è¿‡ç´¢å¼•å­—æ®µ
                if isinstance(value, str) and len(value) > 200:
                    # é•¿æ–‡æœ¬æˆªæ–­
                    value = value[:200] + "..."
                formatted_lines.append(f"{key}: {value}")

    return "\n".join(formatted_lines)


async def extract_table_data(
    url: str,
    table_selector: str = "table",
    context_name: str = "default",
    include_header: bool = True,
    pandas_options: Optional[Dict[str, Any]] = None,
    wait_timeout: int = 10000,
    format_for_llm: bool = True,
    clean_data: bool = True,
    extract_links: bool = False
) -> Dict[str, Any]:
    """
    ä½¿ç”¨pandaså®ç°çš„æˆç†Ÿè¡¨æ ¼æ•°æ®æå–å‡½æ•°ï¼Œæ”¯æŒé«˜çº§æ•°æ®æ¸…æ´—å’Œæ ¼å¼åŒ–

    Args:
        url (str): åŒ…å«è¡¨æ ¼çš„ç½‘é¡µURL
        table_selector (str): è¡¨æ ¼çš„CSSé€‰æ‹©å™¨ï¼Œé»˜è®¤ä¸º"table"
        context_name (str): æµè§ˆå™¨contextåç§°
        include_header (bool): æ˜¯å¦åŒ…å«è¡¨å¤´ï¼Œé»˜è®¤ä¸ºTrue
        pandas_options (Dict[str, Any], optional): pandas.read_htmlçš„å‚æ•°é€‰é¡¹
            - attrs: æ ¹æ®HTMLå±æ€§ç­›é€‰è¡¨æ ¼ {"id": "table1", "class": "data-table"}
            - match: æ ¹æ®æ–‡æœ¬å†…å®¹åŒ¹é…è¡¨æ ¼
            - header: æŒ‡å®šè¡¨å¤´è¡Œå·
            - index_col: æŒ‡å®šç´¢å¼•åˆ—
            - skiprows: è·³è¿‡çš„è¡Œæ•°
            - na_values: è½¬æ¢ä¸ºNaNçš„å€¼
            - keep_default_na: æ˜¯å¦ä¿ç•™é»˜è®¤NaNå€¼
            - converters: åˆ—æ•°æ®è½¬æ¢å™¨
        wait_timeout (int): ç­‰å¾…è¶…æ—¶æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
        format_for_llm (bool): æ˜¯å¦å°†æ•°æ®æ ¼å¼åŒ–ä¸ºä¾¿äºå¤§è¯­è¨€æ¨¡å‹ç†è§£çš„æ–‡æœ¬æ ¼å¼
        clean_data (bool): æ˜¯å¦è¿›è¡Œæ•°æ®æ¸…æ´—ï¼Œé»˜è®¤ä¸ºTrue
        extract_links (bool): æ˜¯å¦æå–è¡¨æ ¼ä¸­çš„é“¾æ¥ï¼Œé»˜è®¤ä¸ºFalse

    Returns:
        Dict[str, Any]: åŒ…å«è¡¨æ ¼æ•°æ®çš„å­—å…¸
            - success (bool): æ˜¯å¦æˆåŠŸ
            - data (Union[str, List[Dict]]): è¡¨æ ¼æ•°æ®
            - raw_data (List[Dict]): åŸå§‹è¡¨æ ¼æ•°æ®
            - pandas_dataframe (Dict): pandas DataFrameä¿¡æ¯ï¼ˆåˆ—åã€æ•°æ®ç±»å‹ç­‰ï¼‰
            - headers (List[str]): è¡¨å¤´åˆ—è¡¨
            - row_count (int): è¡Œæ•°
            - column_count (int): åˆ—æ•°
            - data_quality (Dict): æ•°æ®è´¨é‡æŒ‡æ ‡
            - formatted_summary (str): æ ¼å¼åŒ–æ‘˜è¦
    """
    try:
        LOGGER.info(f"å¼€å§‹ä» {url} æå–è¡¨æ ¼æ•°æ®ï¼ˆpandaså¢å¼ºç‰ˆï¼‰")

        # é»˜è®¤pandasé€‰é¡¹
        default_options = {
            "attrs": None,
            "match": None,
            "header": 0 if include_header else None,
            "index_col": None,
            "skiprows": None,
            "na_values": ["", "N/A", "n/a", "NULL", "null", "-"],
            "keep_default_na": True,
            "converters": None,
            "extract_links": "all" if extract_links else None
        }

        # åˆå¹¶ç”¨æˆ·æä¾›çš„é€‰é¡¹
        if pandas_options:
            default_options.update(pandas_options)

        # è·å–æµè§ˆå™¨ä¸Šä¸‹æ–‡
        context = await context_manager.get_context(context_name)
        page = await context.new_page()

        result_data = {
            "success": False,
            "data": [],
            "raw_data": [],
            "pandas_dataframe": {},
            "headers": [],
            "row_count": 0,
            "column_count": 0,
            "url": url,
            "table_selector": table_selector,
            "message": "",
            "formatted_summary": "",
            "data_quality": {}
        }

        try:
            # è®¿é—®ç½‘é¡µ
            await page.goto(url, wait_until="domcontentloaded", timeout=30000)

            # ç­‰å¾…è¡¨æ ¼åŠ è½½
            await page.wait_for_selector(table_selector, timeout=wait_timeout)

            # æå–è¡¨æ ¼HTMLå†…å®¹
            table_html = await page.locator(table_selector).first.inner_html()

            # æ„å»ºå®Œæ•´çš„è¡¨æ ¼HTML
            full_table_html = f"<table>{table_html}</table>"

            LOGGER.info("å¼€å§‹ä½¿ç”¨pandasè§£æè¡¨æ ¼HTML")

            # ä½¿ç”¨pandasçš„read_htmlæ–¹æ³•è§£æè¡¨æ ¼
            try:
                # åˆ›å»ºpandasè§£æé€‰é¡¹
                pd_options = {}
                for key, value in default_options.items():
                    if value is not None:
                        pd_options[key] = value

                # ä½¿ç”¨pandasè¯»å–HTMLè¡¨æ ¼
                dfs = pd.read_html(StringIO(full_table_html), **pd_options)

                if not dfs:
                    result_data["message"] = "pandasæœªèƒ½è§£æå‡ºä»»ä½•è¡¨æ ¼æ•°æ®"
                    LOGGER.warning(result_data["message"])
                    return result_data

                # å–ç¬¬ä¸€ä¸ªè¡¨æ ¼ï¼ˆä¸»è¦è¡¨æ ¼ï¼‰
                df = dfs[0]

                LOGGER.info(f"pandasæˆåŠŸè§£æè¡¨æ ¼ï¼š{df.shape[0]} è¡Œ x {df.shape[1]} åˆ—")

            except Exception as pandas_error:
                LOGGER.warning(f"pandasè§£æå¤±è´¥ï¼Œå›é€€åˆ°åŸºç¡€æ–¹æ³•: {str(pandas_error)}")
                # å›é€€åˆ°åŸºç¡€æå–æ–¹æ³•
                return await _fallback_table_extraction(
                    page, table_selector, result_data, include_header, format_for_llm, url
                )

            # æ•°æ®æ¸…æ´—
            if clean_data:
                df = _clean_dataframe(df)

            # æå–åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
            data_quality = _analyze_data_quality(df)

            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
            table_data = df.to_dict('records')
            headers = df.columns.tolist()

            # æå–DataFrameå…ƒä¿¡æ¯
            dataframe_info = {
                "columns": headers,
                "dtypes": {col: str(dtype) for col, dtype in df.dtypes.items()},
                "shape": df.shape,
                "memory_usage": df.memory_usage(deep=True).sum(),
                "null_counts": df.isnull().sum().to_dict()
            }

            # è®¾ç½®ç»“æœ
            result_data["success"] = True
            result_data["raw_data"] = table_data
            result_data["pandas_dataframe"] = dataframe_info
            result_data["headers"] = headers
            result_data["row_count"] = len(table_data)
            result_data["column_count"] = len(headers)
            result_data["data_quality"] = data_quality
            result_data["message"] = f"æˆåŠŸæå–è¡¨æ ¼æ•°æ®ï¼š{len(table_data)} è¡Œ x {len(headers)} åˆ—ï¼ˆpandaså¢å¼ºç‰ˆï¼‰"

            # æ ¼å¼åŒ–æ•°æ®
            if format_for_llm:
                result_data["formatted_summary"] = _format_enhanced_table_for_llm(
                    table_data, headers, dataframe_info, data_quality, url
                )
                result_data["data"] = result_data["formatted_summary"]
            else:
                result_data["data"] = table_data

            LOGGER.info(result_data["message"])

        finally:
            await page.close()

        return result_data

    except Exception as e:
        error_msg = f"æå–è¡¨æ ¼æ•°æ®å¤±è´¥: {str(e)}"
        LOGGER.error(error_msg)

        return {
            "success": False,
            "data": "" if format_for_llm else [],
            "raw_data": [],
            "pandas_dataframe": {},
            "headers": [],
            "row_count": 0,
            "column_count": 0,
            "url": url,
            "table_selector": table_selector,
            "message": error_msg,
            "formatted_summary": "",
            "data_quality": {}
        }


def _clean_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    """
    ä½¿ç”¨pandasè¿›è¡Œæ•°æ®æ¸…æ´—

    Args:
        df: åŸå§‹DataFrame

    Returns:
        pd.DataFrame: æ¸…æ´—åçš„DataFrame
    """
    try:
        # åˆ›å»ºå‰¯æœ¬é¿å…ä¿®æ”¹åŸæ•°æ®
        cleaned_df = df.copy()

        # 1. æ¸…ç†åˆ—åï¼šå»é™¤å¤šä½™ç©ºç™½å­—ç¬¦
        cleaned_df.columns = cleaned_df.columns.str.strip()

        # 2. è½¬æ¢æ•°æ®ç±»å‹ï¼šå°è¯•å°†çœ‹èµ·æ¥åƒæ•°å­—çš„åˆ—è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
        for col in cleaned_df.columns:
            if cleaned_df[col].dtype == 'object':
                # å°è¯•è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
                cleaned_df[col] = pd.to_numeric(cleaned_df[col])

        # 3. æ¸…ç†å­—ç¬¦ä¸²æ•°æ®ï¼šå»é™¤å‰åç©ºç™½
        string_columns = cleaned_df.select_dtypes(include=['object']).columns
        for col in string_columns:
            cleaned_df[col] = cleaned_df[col].astype(str).str.strip()

        # 4. å¤„ç†ç©ºå€¼ï¼šç”¨åˆ—çš„ä¼—æ•°å¡«å……æ•°å€¼åˆ—ï¼Œç”¨"æœªçŸ¥"å¡«å……å­—ç¬¦ä¸²åˆ—
        for col in cleaned_df.columns:
            if cleaned_df[col].isnull().any():
                if cleaned_df[col].dtype in ['int64', 'float64']:
                    # æ•°å€¼åˆ—ç”¨ä¸­ä½æ•°å¡«å……
                    median_val = cleaned_df[col].median()
                    cleaned_df[col].fillna(median_val, inplace=True)
                else:
                    # å­—ç¬¦ä¸²åˆ—ç”¨"æœªçŸ¥"å¡«å……
                    cleaned_df[col].fillna("æœªçŸ¥", inplace=True)

        # 5. ç§»é™¤å®Œå…¨ç©ºçš„è¡Œå’Œåˆ—
        cleaned_df.dropna(how='all', inplace=True)
        cleaned_df.dropna(axis=1, how='all', inplace=True)

        LOGGER.info(f"æ•°æ®æ¸…æ´—å®Œæˆï¼š{df.shape} -> {cleaned_df.shape}")
        return cleaned_df

    except Exception as e:
        LOGGER.warning(f"æ•°æ®æ¸…æ´—å¤±è´¥ï¼Œè¿”å›åŸå§‹æ•°æ®: {str(e)}")
        return df


def _analyze_data_quality(df: pd.DataFrame) -> Dict[str, Any]:
    """
    åˆ†ææ•°æ®è´¨é‡

    Args:
        df: DataFrame

    Returns:
        Dict[str, Any]: æ•°æ®è´¨é‡æŒ‡æ ‡
    """
    quality_metrics = {
        "total_rows": len(df),
        "total_columns": len(df.columns),
        "null_count": df.isnull().sum().sum(),
        "duplicate_rows": df.duplicated().sum(),
        "data_types": df.dtypes.value_counts().to_dict(),
        "column_stats": {}
    }

    # åˆ†ææ¯åˆ—çš„ç»Ÿè®¡ä¿¡æ¯
    for col in df.columns:
        col_stats = {
            "data_type": str(df[col].dtype),
            "null_count": df[col].isnull().sum(),
            "unique_count": df[col].nunique(),
            "empty_string_count": (df[col] == '').sum() if df[col].dtype == 'object' else 0
        }

        # æ•°å€¼åˆ—çš„é¢å¤–ç»Ÿè®¡
        if df[col].dtype in ['int64', 'float64']:
            col_stats.update({
                "mean": float(df[col].mean()) if not df[col].empty else None,
                "std": float(df[col].std()) if not df[col].empty else None,
                "min": float(df[col].min()) if not df[col].empty else None,
                "max": float(df[col].max()) if not df[col].empty else None
            })

        # å­—ç¬¦ä¸²åˆ—çš„é¢å¤–ç»Ÿè®¡
        elif df[col].dtype == 'object':
            avg_length = df[col].astype(str).str.len().mean()
            col_stats["avg_string_length"] = float(avg_length) if not pd.isna(avg_length) else 0

        quality_metrics["column_stats"][col] = col_stats

    return quality_metrics


async def _fallback_table_extraction(
    page,
    table_selector: str,
    result_data: Dict[str, Any],
    include_header: bool,
    format_for_llm: bool,
    url: str
) -> Dict[str, Any]:
    """
    å½“pandasè§£æå¤±è´¥æ—¶çš„å›é€€æå–æ–¹æ³•

    Args:
        page: Playwrighté¡µé¢å¯¹è±¡
        table_selector: è¡¨æ ¼é€‰æ‹©å™¨
        result_data: ç»“æœæ•°æ®å­—å…¸
        include_header: æ˜¯å¦åŒ…å«è¡¨å¤´
        format_for_llm: æ˜¯å¦æ ¼å¼åŒ–ä¸ºLLMæ ¼å¼
        url: æºURL

    Returns:
        Dict[str, Any]: æå–ç»“æœ
    """
    try:
        LOGGER.info("ä½¿ç”¨å›é€€æ–¹æ³•æå–è¡¨æ ¼æ•°æ®")

        # å®šä½è¡¨æ ¼
        table_element = page.locator(table_selector).first

        # æå–è¡¨å¤´
        headers = []
        if include_header:
            header_rows = table_element.locator("thead tr, tr:first-child")
            if await header_rows.count() > 0:
                header_cells = header_rows.first.locator("th, td")
                header_count = await header_cells.count()

                for i in range(header_count):
                    header_text = await header_cells.nth(i).text_content()
                    headers.append(header_text.strip() if header_text else f"Column_{i+1}")

        # æå–æ•°æ®è¡Œ
        rows = table_element.locator("tr")
        row_count = await rows.count()

        table_data = []

        for row_idx in range(row_count):
            # è·³è¿‡è¡¨å¤´è¡Œï¼ˆå¦‚æœå·²ç»æå–äº†è¡¨å¤´ï¼‰
            if include_header and row_idx == 0 and await rows.nth(row_idx).locator("th").count() > 0:
                continue

            row_data = {}
            cells = rows.nth(row_idx).locator("td,th")
            cell_count = await cells.count()

            for cell_idx in range(cell_count):
                cell_text = await cells.nth(cell_idx).text_content()
                column_name = headers[cell_idx] if cell_idx < len(headers) else f"Column_{cell_idx+1}"
                row_data[column_name] = cell_text.strip() if cell_text else ""

            if row_data:  # åªæ·»åŠ éç©ºè¡Œ
                table_data.append(row_data)

        # è®¾ç½®ç»“æœ
        result_data["success"] = True
        result_data["raw_data"] = table_data
        result_data["headers"] = headers
        result_data["row_count"] = len(table_data)
        result_data["column_count"] = len(headers) if headers else 0
        result_data["message"] = f"æˆåŠŸæå–è¡¨æ ¼æ•°æ®ï¼ˆå›é€€æ–¹æ³•ï¼‰ï¼š{len(table_data)} è¡Œ x {len(headers) if headers else 0} åˆ—"

        # æ ¼å¼åŒ–æ•°æ®
        if format_for_llm:
            result_data["formatted_summary"] = _format_table_for_llm(table_data, headers, url)
            result_data["data"] = result_data["formatted_summary"]
        else:
            result_data["data"] = table_data

        return result_data

    except Exception as e:
        result_data["message"] = f"å›é€€æ–¹æ³•ä¹Ÿå¤±è´¥: {str(e)}"
        return result_data


def _format_enhanced_table_for_llm(
    table_data: List[Dict[str, Any]],
    headers: List[str],
    dataframe_info: Dict[str, Any],
    data_quality: Dict[str, Any],
    url: str
) -> str:
    """
    å°†å¢å¼ºç‰ˆè¡¨æ ¼æ•°æ®æ ¼å¼åŒ–ä¸ºä¾¿äºå¤§è¯­è¨€æ¨¡å‹ç†è§£çš„æ–‡æœ¬æ ¼å¼

    Args:
        table_data: è¡¨æ ¼æ•°æ®
        headers: è¡¨å¤´
        dataframe_info: DataFrameå…ƒä¿¡æ¯
        data_quality: æ•°æ®è´¨é‡æŒ‡æ ‡
        url: æºURL

    Returns:
        str: æ ¼å¼åŒ–åçš„è¡¨æ ¼æ–‡æœ¬
    """
    if not table_data:
        return "è¡¨æ ¼æ•°æ®ä¸ºç©º"

    summary_parts = [
        "ğŸ“Š è¡¨æ ¼æ•°æ®æå–ç»“æœï¼ˆPandaså¢å¼ºç‰ˆï¼‰",
        f"ğŸ“ æ¥æºURL: {url}",
        f"ğŸ“ è¡¨æ ¼è§„æ¨¡: {data_quality['total_rows']} è¡Œ x {data_quality['total_columns']} åˆ—",
        f"ğŸ’¾ å†…å­˜ä½¿ç”¨: {dataframe_info['memory_usage']:,} å­—èŠ‚",
        f"ğŸ” æ•°æ®è´¨é‡: {data_quality['null_count']} ä¸ªç©ºå€¼, {data_quality['duplicate_rows']} ä¸ªé‡å¤è¡Œ",
        "=" * 60
    ]

    # æ·»åŠ åˆ—ä¿¡æ¯
    summary_parts.append("ğŸ“‹ åˆ—ä¿¡æ¯:")
    for col in headers:
        dtype = dataframe_info['dtypes'][col]
        null_count = data_quality['column_stats'][col]['null_count']
        unique_count = data_quality['column_stats'][col]['unique_count']
        summary_parts.append(f"  â€¢ {col} ({dtype}) - {unique_count} ä¸ªå”¯ä¸€å€¼, {null_count} ä¸ªç©ºå€¼")

    # æ·»åŠ è¡¨å¤´
    summary_parts.append("\nğŸ“‘ è¡¨å¤´:")
    header_line = " | ".join([f"{h:15}" for h in headers])
    summary_parts.append(header_line)
    summary_parts.append("=" * len(header_line))

    # æ·»åŠ æ•°æ®æ ·æœ¬ï¼ˆå‰10è¡Œï¼‰
    summary_parts.append("ğŸ“„ æ•°æ®æ ·æœ¬ï¼ˆå‰10è¡Œï¼‰:")
    sample_data = table_data[:10]
    for i, row in enumerate(sample_data, 1):
        row_values = []
        for header in headers:
            value = str(row.get(header, ""))[:30] + ("..." if len(str(row.get(header, ""))) > 30 else "")
            row_values.append(f"{value:15}")

        row_line = f"è¡Œ{i}: " + " | ".join(row_values)
        summary_parts.append(row_line)

    # æ·»åŠ æ•°å€¼åˆ—ç»Ÿè®¡
    numeric_cols = [col for col, dtype in dataframe_info['dtypes'].items() if 'int' in dtype or 'float' in dtype]
    if numeric_cols:
        summary_parts.append("\nğŸ“ˆ æ•°å€¼åˆ—ç»Ÿè®¡:")
        for col in numeric_cols[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªæ•°å€¼åˆ—
            stats = data_quality['column_stats'][col]
            if stats.get('mean') is not None:
                summary_parts.append(f"  â€¢ {col}: å¹³å‡å€¼={stats['mean']:.2f}, èŒƒå›´=[{stats['min']:.2f}, {stats['max']:.2f}]")

    # æ·»åŠ æ€»ç»“
    summary_parts.extend([
        "=" * 60,
        f"âœ… æ•°æ®æå–å®Œæˆ: å…± {data_quality['total_rows']} è¡Œ x {data_quality['total_columns']} åˆ—æœ‰æ•ˆæ•°æ®",
        f"ğŸ¯ å»ºè®®æ³¨æ„: {'å­˜åœ¨ç¼ºå¤±å€¼' if data_quality['null_count'] > 0 else 'æ•°æ®å®Œæ•´'}, "
        f"{'å­˜åœ¨é‡å¤è¡Œ' if data_quality['duplicate_rows'] > 0 else 'æ— é‡å¤'}"
    ])

    return "\n".join(summary_parts)


def _format_table_for_llm(
    table_data: List[Dict[str, str]],
    headers: List[str],
    url: str
) -> str:
    """
    å°†è¡¨æ ¼æ•°æ®æ ¼å¼åŒ–ä¸ºä¾¿äºå¤§è¯­è¨€æ¨¡å‹ç†è§£çš„æ–‡æœ¬æ ¼å¼ï¼ˆåŸºç¡€ç‰ˆæœ¬ï¼‰

    Args:
        table_data: è¡¨æ ¼æ•°æ®
        headers: è¡¨å¤´
        url: æºURL

    Returns:
        str: æ ¼å¼åŒ–åçš„è¡¨æ ¼æ–‡æœ¬
    """
    if not table_data:
        return "è¡¨æ ¼æ•°æ®ä¸ºç©º"

    summary_parts = [
        f"è¡¨æ ¼æ•°æ®æå–ç»“æœ",
        f"æ¥æºURL: {url}",
        f"è¡¨æ ¼è§„æ¨¡: {len(table_data)} è¡Œ x {len(headers)} åˆ—",
        "-" * 50
    ]

    # æ·»åŠ è¡¨å¤´ä¿¡æ¯
    if headers:
        summary_parts.append("è¡¨å¤´:")
        header_line = " | ".join(headers)
        summary_parts.append(header_line)
        summary_parts.append("-" * len(header_line))

    # æ·»åŠ æ•°æ®è¡Œ
    summary_parts.append("æ•°æ®å†…å®¹:")
    for i, row in enumerate(table_data, 1):
        row_values = []
        for header in headers:
            value = row.get(header, "")
            if value:
                # é™åˆ¶å•å…ƒæ ¼å†…å®¹é•¿åº¦ï¼Œé¿å…è¿‡é•¿
                if len(value) > 100:
                    value = value[:100] + "..."
                row_values.append(value)
            else:
                row_values.append("")

        if any(row_values):  # åªæ˜¾ç¤ºéç©ºè¡Œ
            row_line = f"è¡Œ{i}: {' | '.join(row_values)}"
            summary_parts.append(row_line)

    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    summary_parts.append("-" * 50)
    summary_parts.append(f"æ€»è®¡: {len(table_data)} è¡Œæœ‰æ•ˆæ•°æ®")

    return "\n".join(summary_parts)