# çˆ¬è™«å¼€å‘æŒ‡å—

## ğŸ“– æ¦‚è¿°

FNewsCrawler é‡‡ç”¨æ¨¡å—åŒ–çš„çˆ¬è™«æ¶æ„è®¾è®¡ï¼ŒåŸºäº Playwright æµè§ˆå™¨è‡ªåŠ¨åŒ–æŠ€æœ¯ï¼Œä¸ºè´¢ç»æ•°æ®é‡‡é›†æä¾›é«˜æ•ˆã€ç¨³å®šçš„è§£å†³æ–¹æ¡ˆã€‚æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»äº†çˆ¬è™«ç³»ç»Ÿçš„è®¾è®¡ç†å¿µã€æ¶æ„ç»„ä»¶å’Œå¼€å‘è§„èŒƒã€‚

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### æ ¸å¿ƒè®¾è®¡ç†å¿µ

1. **åŸŸåç»„ç»‡åŸåˆ™**ï¼šæ‰€æœ‰çˆ¬è™«æŒ‰ç›®æ ‡ç½‘ç«™åŸŸåè¿›è¡Œç»„ç»‡ï¼Œç¡®ä¿ä»£ç ç»“æ„æ¸…æ™°
2. **å•ä¾‹æ¨¡å¼ç®¡ç†**ï¼šé‡‡ç”¨å•ä¾‹æ¨¡å¼ç®¡ç†æµè§ˆå™¨å®ä¾‹ï¼Œæé«˜èµ„æºåˆ©ç”¨æ•ˆç‡
3. **ä¸Šä¸‹æ–‡å…±äº«æœºåˆ¶**ï¼šåŒåŸŸåä¸‹çš„æ‰€æœ‰æ“ä½œå…±äº«æµè§ˆå™¨ä¸Šä¸‹æ–‡ï¼Œç»´æŠ¤ä¼šè¯çŠ¶æ€
4. **ç»Ÿä¸€ç™»å½•æ¡†æ¶**ï¼šæä¾›æ ‡å‡†åŒ–çš„äºŒç»´ç ç™»å½•åŸºç±»ï¼Œç®€åŒ–ç™»å½•å®ç°
5. **å¼‚æ­¥ä¼˜å…ˆè®¾è®¡**ï¼šå…¨é¢é‡‡ç”¨å¼‚æ­¥ç¼–ç¨‹æ¨¡å¼ï¼Œæ”¯æŒé«˜å¹¶å‘è®¿é—®

### ç³»ç»Ÿæ¶æ„å›¾

```
çˆ¬è™«ç³»ç»Ÿæ¶æ„
â”œâ”€â”€ ğŸŒ BrowserManager (å•ä¾‹)
â”‚   â”œâ”€â”€ Playwright å®ä¾‹ç®¡ç†
â”‚   â”œâ”€â”€ æµè§ˆå™¨ç”Ÿå‘½å‘¨æœŸæ§åˆ¶
â”‚   â”œâ”€â”€ å¥åº·æ£€æŸ¥ä¸è‡ªåŠ¨æ¢å¤
â”‚   â””â”€â”€ èµ„æºæ¸…ç†ä¸ä¼˜åŒ–
â”‚
â”œâ”€â”€ ğŸ”„ ContextManager (å•ä¾‹)
â”‚   â”œâ”€â”€ åŸŸåçº§ä¸Šä¸‹æ–‡éš”ç¦»
â”‚   â”œâ”€â”€ ä¼šè¯çŠ¶æ€æŒä¹…åŒ–
â”‚   â”œâ”€â”€ è‡ªåŠ¨æ¸…ç†è¿‡æœŸä¸Šä¸‹æ–‡
â”‚   â””â”€â”€ å¹¶å‘è®¿é—®æ§åˆ¶
â”‚
â”œâ”€â”€ ğŸ” QRLoginBase (æŠ½è±¡åŸºç±»)
â”‚   â”œâ”€â”€ äºŒç»´ç è·å–æ¥å£
â”‚   â”œâ”€â”€ ç™»å½•çŠ¶æ€éªŒè¯
â”‚   â”œâ”€â”€ ä¼šè¯çŠ¶æ€ä¿å­˜
â”‚   â””â”€â”€ ç™»å½•çŠ¶æ€ç®¡ç†
â”‚
â””â”€â”€ ğŸ•·ï¸ Spider Modules (æŒ‰åŸŸåç»„ç»‡)
    â”œâ”€â”€ iwencai/
    â”‚   â”œâ”€â”€ login.py    # ç™»å½•å®ç°
    â”‚   â”œâ”€â”€ crawl.py    # çˆ¬è™«é€»è¾‘
    â”‚   â””â”€â”€ __init__.py # æ¨¡å—å¯¼å‡º
    â”œâ”€â”€ eastmoney/
    â””â”€â”€ å…¶ä»–ç«™ç‚¹.../
```

## ğŸ§© æ ¸å¿ƒç»„ä»¶è¯¦è§£

### 1. BrowserManager - æµè§ˆå™¨ç®¡ç†å™¨

**ä½ç½®**: `fnewscrawler/core/browser.py`

**è®¾è®¡ç‰¹ç‚¹**:
- å•ä¾‹æ¨¡å¼ç¡®ä¿å…¨å±€å”¯ä¸€æµè§ˆå™¨å®ä¾‹
- æ”¯æŒ headless/headed æ¨¡å¼åˆ‡æ¢
- è‡ªåŠ¨å¥åº·æ£€æŸ¥å’Œæ•…éšœæ¢å¤
- ä¼˜é›…çš„èµ„æºæ¸…ç†æœºåˆ¶

**æ ¸å¿ƒæ–¹æ³•**:
```python
class BrowserManager:
    async def initialize(self) -> None:
        """åˆå§‹åŒ–æµè§ˆå™¨å®ä¾‹"""
    
    async def get_browser(self) -> Browser:
        """è·å–æµè§ˆå™¨å®ä¾‹ï¼Œè‡ªåŠ¨å¤„ç†é‡è¿"""
    
    async def get_browser_info(self) -> dict:
        """è·å–æµè§ˆå™¨çŠ¶æ€ä¿¡æ¯"""
    
    async def force_restart(self) -> None:
        """å¼ºåˆ¶é‡å¯æµè§ˆå™¨"""
```

**ä½¿ç”¨ç¤ºä¾‹**:
```python
from fnewscrawler.core.browser import browser_manager

# è·å–æµè§ˆå™¨å®ä¾‹
browser = await browser_manager.get_browser()
```

### 2. ContextManager - ä¸Šä¸‹æ–‡ç®¡ç†å™¨

**ä½ç½®**: `fnewscrawler/core/context.py`

**è®¾è®¡ç‰¹ç‚¹**:
- æŒ‰åŸŸåéš”ç¦»æµè§ˆå™¨ä¸Šä¸‹æ–‡
- è‡ªåŠ¨ç»´æŠ¤ç™»å½•çŠ¶æ€å’ŒCookie
- æ”¯æŒä¸Šä¸‹æ–‡çš„åˆ›å»ºã€è·å–ã€åˆ·æ–°å’Œæ¸…ç†
- RedisæŒä¹…åŒ–å­˜å‚¨ä¼šè¯çŠ¶æ€

**æ ¸å¿ƒæ–¹æ³•**:
```python
class ContextManager:
    async def get_context(self, site_name: str, force_new: bool = False) -> BrowserContext:
        """è·å–æŒ‡å®šç«™ç‚¹çš„æµè§ˆå™¨ä¸Šä¸‹æ–‡"""
    
    async def save_context_state(self, site_name: str) -> bool:
        """ä¿å­˜ä¸Šä¸‹æ–‡çŠ¶æ€åˆ°Redis"""
    
    async def refresh_context(self, site_name: str) -> BrowserContext:
        """åˆ·æ–°æŒ‡å®šç«™ç‚¹çš„ä¸Šä¸‹æ–‡"""
    
    async def get_context_stats(self) -> Dict[str, Any]:
        """è·å–ä¸Šä¸‹æ–‡ç»Ÿè®¡ä¿¡æ¯"""
```

**ä½¿ç”¨ç¤ºä¾‹**:
```python
from fnewscrawler.core.context import context_manager

# è·å–iwencaiç«™ç‚¹çš„ä¸Šä¸‹æ–‡
context = await context_manager.get_context("iwencai")
page = await context.new_page()
```

### 3. QRLoginBase - äºŒç»´ç ç™»å½•åŸºç±»

**ä½ç½®**: `fnewscrawler/core/qr_login_base.py`

**è®¾è®¡ç‰¹ç‚¹**:
- æŠ½è±¡åŸºç±»å®šä¹‰æ ‡å‡†ç™»å½•æ¥å£
- å•ä¾‹æ¨¡å¼å‡å°‘å®ä¾‹åŒ–å¼€é”€
- ç»Ÿä¸€çš„ç™»å½•æµç¨‹å’ŒçŠ¶æ€ç®¡ç†
- æ”¯æŒå¤šç§äºŒç»´ç ç™»å½•æ–¹å¼

**æŠ½è±¡æ–¹æ³•**:
```python
class QRLoginBase(ABC):
    @abstractmethod
    async def get_qr_code(self, qr_type: str = "å¾®ä¿¡") -> Tuple[bool, str]:
        """è·å–ç™»å½•äºŒç»´ç """
    
    @abstractmethod
    async def verify_login_success(self) -> bool:
        """éªŒè¯ç™»å½•æ˜¯å¦æˆåŠŸ"""
    
    @abstractmethod
    async def save_context_state(self):
        """ä¿å­˜æµè§ˆå™¨çŠ¶æ€åˆ°Redis"""
    
    @abstractmethod
    async def get_login_status(self) -> bool:
        """è·å–å½“å‰ç™»å½•çŠ¶æ€"""
    
    @abstractmethod
    async def clean_login_state(self) -> bool:
        """æ¸…ç†ç™»å½•çŠ¶æ€"""
    
    @abstractmethod
    def get_supported_qr_types(self) -> List[str]:
        """è·å–æ”¯æŒçš„äºŒç»´ç ç±»å‹"""
```

## ğŸ“ ç›®å½•ç»“æ„è§„èŒƒ

### çˆ¬è™«æ¨¡å—ç»„ç»‡

```
fnewscrawler/spiders/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ iwencai/                    # åŒèŠ±é¡ºé—®è´¢
â”‚   â”œâ”€â”€ __init__.py            # æ¨¡å—å¯¼å‡º
â”‚   â”œâ”€â”€ login.py               # ç™»å½•å®ç°
â”‚   â”œâ”€â”€ crawl.py               # çˆ¬è™«é€»è¾‘
â”‚   â””â”€â”€ utils.py               # å·¥å…·å‡½æ•°ï¼ˆå¯é€‰ï¼‰
â”œâ”€â”€ eastmoney/                  # ä¸œæ–¹è´¢å¯Œ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ login.py
â”‚   â”œâ”€â”€ crawl.py
â”‚   â””â”€â”€ utils.py
â””â”€â”€ æ–°ç«™ç‚¹/                     # æ–°å¢ç«™ç‚¹
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ login.py               # å¦‚éœ€ç™»å½•
    â”œâ”€â”€ crawl.py
    â””â”€â”€ utils.py
```

### æ–‡ä»¶å‘½åè§„èŒƒ

- **login.py**: ç™»å½•ç›¸å…³é€»è¾‘ï¼Œç»§æ‰¿ `QRLoginBase`
- **crawl.py**: æ ¸å¿ƒçˆ¬è™«é€»è¾‘ï¼Œæ•°æ®æå–å’Œå¤„ç†
- **utils.py**: ç«™ç‚¹ç‰¹å®šçš„å·¥å…·å‡½æ•°ï¼ˆå¯é€‰ï¼‰
- **__init__.py**: æ¨¡å—å¯¼å‡ºï¼Œæš´éœ²ä¸»è¦æ¥å£

## ğŸ› ï¸ å¼€å‘å®è·µ

### 1. åˆ›å»ºæ–°çˆ¬è™«æ¨¡å—

**æ­¥éª¤1: åˆ›å»ºç›®å½•ç»“æ„**
```bash
mkdir fnewscrawler/spiders/æ–°ç«™ç‚¹å
touch fnewscrawler/spiders/æ–°ç«™ç‚¹å/__init__.py
touch fnewscrawler/spiders/æ–°ç«™ç‚¹å/crawl.py
touch fnewscrawler/spiders/æ–°ç«™ç‚¹å/login.py  # å¦‚éœ€ç™»å½•
```

**æ­¥éª¤2: å®ç°ç™»å½•ç±»ï¼ˆå¦‚éœ€è¦ï¼‰**
```python
# fnewscrawler/spiders/æ–°ç«™ç‚¹å/login.py
from typing import Tuple, List
from fnewscrawler.core.qr_login_base import QRLoginBase
from fnewscrawler.utils.logger import LOGGER

class NewSiteLogin(QRLoginBase):
    def __init__(self):
        super().__init__()
        self.base_url = "https://æ–°ç«™ç‚¹åŸŸå.com"
        self.login_page = None
    
    async def get_qr_code(self, qr_type: str = "å¾®ä¿¡") -> Tuple[bool, str]:
        """è·å–ç™»å½•äºŒç»´ç """
        try:
            # è·å–ç«™ç‚¹ä¸Šä¸‹æ–‡
            context = await self.context_manager.get_context("æ–°ç«™ç‚¹å")
            self.login_page = await context.new_page()
            
            # å¯¼èˆªåˆ°ç™»å½•é¡µé¢
            await self.login_page.goto(self.base_url)
            
            # ç‚¹å‡»ç™»å½•æŒ‰é’®
            await self.login_page.click("ç™»å½•æŒ‰é’®é€‰æ‹©å™¨")
            
            # é€‰æ‹©äºŒç»´ç ç™»å½•æ–¹å¼
            if qr_type == "å¾®ä¿¡":
                await self.login_page.click("å¾®ä¿¡ç™»å½•é€‰æ‹©å™¨")
            
            # ç­‰å¾…äºŒç»´ç åŠ è½½
            await self.login_page.wait_for_selector("äºŒç»´ç é€‰æ‹©å™¨")
            
            # è·å–äºŒç»´ç URL
            qr_element = self.login_page.locator("äºŒç»´ç é€‰æ‹©å™¨")
            qr_url = await qr_element.get_attribute("src")
            
            return True, qr_url
            
        except Exception as e:
            LOGGER.error(f"è·å–äºŒç»´ç å¤±è´¥: {e}")
            return False, str(e)
    
    async def verify_login_success(self) -> bool:
        """éªŒè¯ç™»å½•æ˜¯å¦æˆåŠŸ"""
        try:
            # ç­‰å¾…ç™»å½•æˆåŠŸçš„æ ‡å¿—å…ƒç´ 
            await self.login_page.wait_for_selector(
                "ç™»å½•æˆåŠŸæ ‡å¿—é€‰æ‹©å™¨", 
                timeout=30000
            )
            return True
        except Exception as e:
            LOGGER.warning(f"ç™»å½•éªŒè¯å¤±è´¥: {e}")
            return False
    
    async def save_context_state(self):
        """ä¿å­˜æµè§ˆå™¨çŠ¶æ€"""
        await self.context_manager.save_context_state("æ–°ç«™ç‚¹å")
    
    async def get_login_status(self) -> bool:
        """è·å–ç™»å½•çŠ¶æ€"""
        try:
            context = await self.context_manager.get_context("æ–°ç«™ç‚¹å")
            page = await context.new_page()
            await page.goto(self.base_url)
            
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç™»å½•çŠ¶æ€æ ‡å¿—
            is_logged_in = await page.locator("ç™»å½•çŠ¶æ€æ ‡å¿—é€‰æ‹©å™¨").count() > 0
            await page.close()
            
            return is_logged_in
        except Exception as e:
            LOGGER.error(f"æ£€æŸ¥ç™»å½•çŠ¶æ€å¤±è´¥: {e}")
            return False
    
    async def clean_login_state(self) -> bool:
        """æ¸…ç†ç™»å½•çŠ¶æ€"""
        try:
            await self.context_manager.delete_context_state("æ–°ç«™ç‚¹å")
            return True
        except Exception as e:
            LOGGER.error(f"æ¸…ç†ç™»å½•çŠ¶æ€å¤±è´¥: {e}")
            return False
    
    def get_supported_qr_types(self) -> List[str]:
        """è·å–æ”¯æŒçš„äºŒç»´ç ç±»å‹"""
        return ["å¾®ä¿¡", "QQ"]  # æ ¹æ®å®é™…æ”¯æŒçš„ç±»å‹è°ƒæ•´
```

**æ­¥éª¤3: å®ç°çˆ¬è™«é€»è¾‘**
```python
# fnewscrawler/spiders/æ–°ç«™ç‚¹å/crawl.py
from typing import List, Dict, Optional
from fnewscrawler.core.context import context_manager
from fnewscrawler.utils.logger import LOGGER

async def new_site_crawl_from_query(query: str, page_no: int = 1) -> List[Dict[str, str]]:
    """ä»æ–°ç«™ç‚¹è·å–æ•°æ®"""
    try:
        # è·å–ç«™ç‚¹ä¸Šä¸‹æ–‡
        context = await context_manager.get_context("æ–°ç«™ç‚¹å")
        page = await context.new_page()
        
        # æ„å»ºæœç´¢URL
        search_url = f"https://æ–°ç«™ç‚¹åŸŸå.com/search?q={query}&page={page_no}"
        
        # å¯¼èˆªåˆ°æœç´¢é¡µé¢
        await page.goto(search_url)
        await page.wait_for_load_state("domcontentloaded")
        
        # æå–æ•°æ®
        results = []
        items = page.locator("æ•°æ®é¡¹é€‰æ‹©å™¨")
        count = await items.count()
        
        for i in range(count):
            item = items.nth(i)
            
            # æå–æ ‡é¢˜
            title_element = item.locator("æ ‡é¢˜é€‰æ‹©å™¨")
            title = await title_element.text_content() if await title_element.count() > 0 else ""
            
            # æå–é“¾æ¥
            link_element = item.locator("é“¾æ¥é€‰æ‹©å™¨")
            link = await link_element.get_attribute("href") if await link_element.count() > 0 else ""
            
            # æå–æ—¶é—´
            time_element = item.locator("æ—¶é—´é€‰æ‹©å™¨")
            time = await time_element.text_content() if await time_element.count() > 0 else ""
            
            # æå–æ¥æº
            source_element = item.locator("æ¥æºé€‰æ‹©å™¨")
            source = await source_element.text_content() if await source_element.count() > 0 else ""
            
            results.append({
                "title": title.strip(),
                "url": link,
                "time": time.strip(),
                "source": source.strip(),
                "content": ""  # å¦‚éœ€è¦å¯è¿›ä¸€æ­¥æå–å†…å®¹
            })
        
        await page.close()
        return results
        
    except Exception as e:
        LOGGER.error(f"çˆ¬å–æ–°ç«™ç‚¹æ•°æ®å¤±è´¥: {e}")
        return []
```

**æ­¥éª¤4: é…ç½®æ¨¡å—å¯¼å‡º**
```python
# fnewscrawler/spiders/æ–°ç«™ç‚¹å/__init__.py
from .crawl import new_site_crawl_from_query
from .login import NewSiteLogin  # å¦‚æœ‰ç™»å½•åŠŸèƒ½

__all__ = ["new_site_crawl_from_query", "NewSiteLogin"]
```

### 2. æœ€ä½³å®è·µ

#### é”™è¯¯å¤„ç†
```python
try:
    # çˆ¬è™«é€»è¾‘
    pass
except Exception as e:
    LOGGER.error(f"æ“ä½œå¤±è´¥: {e}")
    # é€‚å½“çš„é”™è¯¯æ¢å¤é€»è¾‘
    return []  # æˆ–å…¶ä»–é»˜è®¤å€¼
```

#### ç­‰å¾…ç­–ç•¥
```python
# ç­‰å¾…é¡µé¢åŠ è½½
await page.wait_for_load_state("domcontentloaded")

# ç­‰å¾…ç‰¹å®šå…ƒç´ 
await page.wait_for_selector("é€‰æ‹©å™¨", timeout=10000)

# ç­‰å¾…ç½‘ç»œç©ºé—²
await page.wait_for_load_state("networkidle")
```

#### æ•°æ®æå–
```python
# å®‰å…¨çš„æ–‡æœ¬æå–
title = await element.text_content() if await element.count() > 0 else ""

# å®‰å…¨çš„å±æ€§æå–
link = await element.get_attribute("href") if await element.count() > 0 else ""

# æ‰¹é‡å…ƒç´ å¤„ç†
items = page.locator("é€‰æ‹©å™¨")
count = await items.count()
for i in range(count):
    item = items.nth(i)
    # å¤„ç†å•ä¸ªå…ƒç´ 
```

#### èµ„æºç®¡ç†
```python
# åŠæ—¶å…³é—­é¡µé¢
try:
    page = await context.new_page()
    # é¡µé¢æ“ä½œ
finally:
    await page.close()

# æˆ–ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨
async with context.new_page() as page:
    # é¡µé¢æ“ä½œ
    pass  # è‡ªåŠ¨å…³é—­
```

## ğŸ” è°ƒè¯•æŠ€å·§

### 1. å¯ç”¨å¯è§†åŒ–æ¨¡å¼
```bash
# è®¾ç½®ç¯å¢ƒå˜é‡
export PW_USE_HEADLESS=false

# æˆ–åœ¨ä»£ç ä¸­è®¾ç½®
os.environ["PW_USE_HEADLESS"] = "false"
```

### 2. æ·»åŠ è°ƒè¯•æ—¥å¿—
```python
from fnewscrawler.utils.logger import LOGGER

LOGGER.debug(f"å½“å‰é¡µé¢URL: {page.url}")
LOGGER.info(f"æ‰¾åˆ° {count} ä¸ªæ•°æ®é¡¹")
```

### 3. æˆªå›¾è°ƒè¯•
```python
# ä¿å­˜é¡µé¢æˆªå›¾
await page.screenshot(path="debug.png")

# ä¿å­˜ç‰¹å®šå…ƒç´ æˆªå›¾
await element.screenshot(path="element.png")
```

### 4. é¡µé¢å†…å®¹æ£€æŸ¥
```python
# æ‰“å°é¡µé¢HTML
html = await page.content()
print(html)

# æ£€æŸ¥å…ƒç´ æ˜¯å¦å­˜åœ¨
if await page.locator("é€‰æ‹©å™¨").count() > 0:
    print("å…ƒç´ å­˜åœ¨")
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### 1. ä¸Šä¸‹æ–‡å¤ç”¨
```python
# å¥½çš„åšæ³•ï¼šå¤ç”¨ä¸Šä¸‹æ–‡
context = await context_manager.get_context("ç«™ç‚¹å")
for query in queries:
    page = await context.new_page()
    # å¤„ç†æŸ¥è¯¢
    await page.close()

# é¿å…ï¼šé¢‘ç¹åˆ›å»ºæ–°ä¸Šä¸‹æ–‡
for query in queries:
    context = await context_manager.get_context("ç«™ç‚¹å", force_new=True)  # ä¸æ¨è
```

### 2. å¹¶å‘æ§åˆ¶
```python
import asyncio
from asyncio import Semaphore

# é™åˆ¶å¹¶å‘æ•°
semaphore = Semaphore(5)  # æœ€å¤š5ä¸ªå¹¶å‘

async def crawl_with_limit(query):
    async with semaphore:
        return await crawl_function(query)

# æ‰¹é‡å¤„ç†
tasks = [crawl_with_limit(query) for query in queries]
results = await asyncio.gather(*tasks)
```

### 3. èµ„æºæ¸…ç†
```python
# å®šæœŸæ¸…ç†è¿‡æœŸä¸Šä¸‹æ–‡
await context_manager._cleanup_expired_contexts()

# æ‰‹åŠ¨æ¸…ç†ç‰¹å®šä¸Šä¸‹æ–‡
await context_manager.close_site_context("ç«™ç‚¹å")
```

## ğŸ§ª æµ‹è¯•æŒ‡å—

### 1. å•å…ƒæµ‹è¯•
```python
import pytest
from fnewscrawler.spiders.æ–°ç«™ç‚¹å import new_site_crawl_from_query

@pytest.mark.asyncio
async def test_crawl_function():
    results = await new_site_crawl_from_query("æµ‹è¯•æŸ¥è¯¢")
    assert isinstance(results, list)
    if results:
        assert "title" in results[0]
        assert "url" in results[0]
```

### 2. é›†æˆæµ‹è¯•
```python
@pytest.mark.asyncio
async def test_login_flow():
    login = NewSiteLogin()
    
    # æµ‹è¯•äºŒç»´ç è·å–
    success, qr_url = await login.get_qr_code()
    assert success
    assert qr_url.startswith("http")
    
    # æµ‹è¯•ç™»å½•çŠ¶æ€æ£€æŸ¥
    status = await login.get_login_status()
    assert isinstance(status, bool)
```

## ğŸ“‹ æ£€æŸ¥æ¸…å•

å¼€å‘æ–°çˆ¬è™«æ—¶ï¼Œè¯·ç¡®ä¿å®Œæˆä»¥ä¸‹æ£€æŸ¥ï¼š

- [ ] æŒ‰åŸŸååˆ›å»ºç›®å½•ç»“æ„
- [ ] å®ç°å¿…è¦çš„ç™»å½•ç±»ï¼ˆå¦‚éœ€è¦ï¼‰
- [ ] å®ç°æ ¸å¿ƒçˆ¬è™«é€»è¾‘
- [ ] é…ç½®æ¨¡å—å¯¼å‡º
- [ ] æ·»åŠ é€‚å½“çš„é”™è¯¯å¤„ç†
- [ ] ç¼–å†™å•å…ƒæµ‹è¯•
- [ ] æµ‹è¯•ç™»å½•æµç¨‹ï¼ˆå¦‚æœ‰ï¼‰
- [ ] æµ‹è¯•æ•°æ®æå–åŠŸèƒ½
- [ ] éªŒè¯èµ„æºæ¸…ç†
- [ ] æ€§èƒ½æµ‹è¯•å’Œä¼˜åŒ–

---

é€šè¿‡éµå¾ªæœ¬æŒ‡å—ï¼Œæ‚¨å¯ä»¥é«˜æ•ˆåœ°å¼€å‘å‡ºç¨³å®šã€å¯ç»´æŠ¤çš„è´¢ç»æ•°æ®çˆ¬è™«æ¨¡å—ã€‚å¦‚æœ‰ç–‘é—®ï¼Œè¯·å‚è€ƒç°æœ‰çš„ `iwencai` æ¨¡å—å®ç°æˆ–æäº¤ Issue å¯»æ±‚å¸®åŠ©ã€‚